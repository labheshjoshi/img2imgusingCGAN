Image translation using cGAN

Image-to-Image Translation is a task in computer vision and machine learning where the goal is to learn a mapping between an input image and an output image, such that the output image can be used to perform a specific task, such as style transfer, data augmentation, or image restoration.

Generator is a U-net with skipp connection and Discriminator is a patchGAN. The input to the generator is a series of randomly generated numbers called latent sample. Discriminator is trained using both the original dataset and the images generated by the generator. If the input is from the original dataset, then discriminator should classify it as real and if the input is from the generator, then it should classify it as fake.

Objective of the cGAN is:

![image](https://github.com/user-attachments/assets/ac37ef27-bf08-4a8d-ad0c-a944a9425f2d)


Prerequisite:
Keras
Tensorflow 2.0
Numpy
Matplotlib

Training:
cGAN model trained for 100 epochs with batch size as 1. And generated images and weights of the model are saved after every 10 epochs. Once the training is done, these generated images are comapred with the target images, and the one which produces the more realistic images is selected as a final model.

In this case, I could achieve better results after 60 epochs. And the same result is shown below.

output

![image](https://github.com/user-attachments/assets/7261b064-d9e0-4dea-992f-da7c735396c5)
